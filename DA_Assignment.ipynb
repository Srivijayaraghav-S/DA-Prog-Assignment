{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Implementing SON and Toivonen's Algorithm Using MapReduce**\n"
      ],
      "metadata": {
        "id": "pU4HH4MaZYi7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The SON algorithm and the Toivonen algorithm are both widely used approaches for solving the problem of frequent itemset mining in large datasets. These algorithms are particularly well-suited to distributed computing environments, where data is too large to fit into the memory of a single machine.\n",
        "\n",
        "Implementing these algorithms in MapReduce or Apache Spark leverages the power of distributed computing to handle very large datasets efficiently. In a MapReduce context, the mapper function can distribute data partitions across nodes for local frequent itemset mining (Phase 1 of SON or the sampling step of Toivonen), while the reducer function aggregates the results across all nodes for the global verification step. Apache Spark, with its in-memory computing capabilities, offers a more efficient and faster platform for these algorithms, especially due to its optimization for iterative algorithms like Apriori, which is used within both SON and Toivonen algorithms, while Spark's resilient distributed datasets (RDDs) and dataframes provide flexible abstractions for distributing data and computations."
      ],
      "metadata": {
        "id": "9NZ_0uv2b-PM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **SON Algorithm**\n",
        "The SON algorithm, proposed by Savasere, Omiecinski, and Navathe, breaks down the task of identifying frequent itemsets into two phases to make it manageable across distributed systems. In the first phase, the algorithm partitions the dataset and applies the Apriori algorithm to each partition to find local frequent itemsets. This phase significantly reduces the dataset's size that each node must handle, allowing the algorithm to scale efficiently with data size. In the second phase, the algorithm aggregates the local frequent itemsets from all partitions and then scans the entire dataset to determine which of these itemsets are indeed frequent across the whole dataset. The SON algorithm's is simple and effective, enabling parallel processing without missing any frequent itemsets."
      ],
      "metadata": {
        "id": "CLVyUEBtfQRc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Working of SON Algorithm**\n",
        "\n",
        "## First Pass\n",
        "*   Repeatedly read small subsets of the baskets into main memory\n",
        "*   Run an in-memory algorithm (e.g., Apriori, random sampling) to find all frequent itemsets\\\n",
        "(Note: we are not sampling, but processing the entire file in memory-sized chunks)\n",
        "*   An itemset becomes a candidate if it is found to be frequent in any one or more subsets of the baskets\n",
        "\n",
        "## Second Pass\n",
        "*   Count all the candidate itemsets and determine which are frequent in the entire set\n",
        "*   Key **“monotonicity”** idea: an itemset cannot be frequent in the entire set of baskets unless it is frequent in at least one subset\n",
        "*   Subset or chunk contains fraction *p* of whole file\n",
        "*   *1/p* chunks in file\n",
        "*   If itemset is not frequent in any chunk, then support in each chunk is less than *ps*\n",
        "*   Support in whole file is less than *s*: not frequent\n",
        "\n",
        "## **SON: MapReduce**\n",
        "## Phase 1: Find Candidate Itemsets\n",
        "**Map:**\n",
        "*   Input is a chunk/subset of all baskets - fraction *p* of total input file\n",
        "*   Find itemsets frequent in that subset (e.g., using Apriori algorithm)\n",
        "*   Use support threshold *ps*\n",
        "*   Output is set of key-value pairs (*F*, 1), where *F* is a frequent itemset from sample\n",
        "\n",
        "**Reduce:**\n",
        "*   Each reduce task is assigned set of keys, which are itemsets\n",
        "*   Produces keys that appear one or more time\n",
        "*   Frequent in some subset\n",
        "*   These are candidate itemsets\n",
        "\n",
        "## Phase 2: Find True Frequent Itemsets\n",
        "**Map:**\n",
        "*   Each Map task takes output from first Reduce task AND a chunk of the total input data file\n",
        "*   All candidate itemsets go to every Map task\n",
        "*   Count occurrences of each candidate itemset among the baskets in the input chunk\n",
        "*   Output is set of key-value pairs (*C*, *v*), where *C* is a candidate frequent itemset and *v* is the support for that itemset among the baskets in the input chunk\n",
        "\n",
        "**Reduce:**\n",
        "*   Each reduce tasks is assigned a set of keys (itemsets)\n",
        "*   Sums associated values for each key: total support for itemset\n",
        "*   If support of itemset >= *s*, print itemset and its count\n",
        "\n",
        "However, even with SON algorithm, we still don’t know whether we found all the frequent itemsets, as an itemset may be infrequent in all subsets but frequent overall - Toivonen's algorithm solves this."
      ],
      "metadata": {
        "id": "GPgKy0BzgX61"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install pyspark\n",
        "# Apache Spark is an open-source, distributed processing system used for big data workloads\n",
        "!pip install pyspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-9Ki8qwEZfcD",
        "outputId": "d9d8dc64-081c-4467-c8ba-78fe7e865467"
      },
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.10/dist-packages (3.5.1)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 130,
      "metadata": {
        "id": "6Rxlw2Ra0c3c"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import pyspark\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark import SparkContext, SparkConf\n",
        "from collections import Counter\n",
        "from itertools import combinations\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize a Spark session\n",
        "# The appName method names the application as 'SON_DM'\n",
        "spark = SparkSession.builder.appName('SON_DM').getOrCreate()"
      ],
      "metadata": {
        "id": "fN94zdO2bn2m"
      },
      "execution_count": 131,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the input file location\n",
        "inputFile = \"basket.txt\"\n",
        "\n",
        "# Create an RDD (Resilient Distributed Dataset) by reading the text file (RDDs are the fundamental data structure of Spark)\n",
        "rdd = spark.sparkContext.textFile(inputFile)"
      ],
      "metadata": {
        "id": "VucT5PQ2bprP"
      },
      "execution_count": 132,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the first 10 lines of the RDD to check data loading\n",
        "print(rdd.take(10))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8RtKLOt6bteP",
        "outputId": "bec64237-8c50-4cd8-8b4f-1f2159b1d7ae"
      },
      "execution_count": 133,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['1,2,4,10,14,15,16', '1,2,4,5,14,16,17', '1,3,4,5,6,7,8,14,17,19', '1,2,3,4,5,12,13,14,15,16', '1,3,4,6,7,13,14,15,16', '1,2,4,5,8,9,13,14,15,16,17,18', '1,2,3,6,7,9,14,16,17', '1,2,3,5,8,14,15,19', '1,2,3,4,6,8,13', '1,3,4,5,6,12,14,15,16,18']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to count the occurrence of each candidate itemset in the dataset\n",
        "def total_c_Count(baskets, candidates):\n",
        "    item_dict = {}\n",
        "\n",
        "    # Convert the iterator to a list for processing\n",
        "    baskets = list(baskets)\n",
        "\n",
        "    # Iterate over each candidate itemset\n",
        "    for candidate in candidates:\n",
        "        # Ensure the candidate is in a consistent format (as a tuple)\n",
        "        if type(candidate) is int:\n",
        "            candidate = [candidate]\n",
        "            key = tuple(sorted(candidate))\n",
        "        else:\n",
        "            key = candidate\n",
        "        candidate = set(candidate)\n",
        "\n",
        "        # Check if the candidate is a subset of each basket and count occurrences\n",
        "        for basket in baskets:\n",
        "            if candidate.issubset(basket):\n",
        "                if key in  item_dict:\n",
        "                     item_dict[key] =  item_dict[key] + 1\n",
        "                else:\n",
        "                     item_dict[key] = 1\n",
        "    return item_dict.items()"
      ],
      "metadata": {
        "id": "_pekmWhh2oUV"
      },
      "execution_count": 134,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apriori algorithm implementation for the first phase of SON\n",
        "# It finds locally frequent itemsets in each partition\n",
        "def apriori(baskets, support, chunkCount):\n",
        "    localRes = list()\n",
        "\n",
        "    # Convert the iterator to a list\n",
        "    baskets = list(baskets)\n",
        "\n",
        "    # Local support threshold\n",
        "    threshold = support*(float(len(baskets))/float(chunkCount))\n",
        "\n",
        "    # Count occurrences of single items\n",
        "    singleton = Counter()\n",
        "    for basket in baskets:\n",
        "        singleton.update(basket)\n",
        "\n",
        "    # Filter singletons by local support threshold\n",
        "    c_singletons = {x : singleton[x] for x in singleton if singleton[x] >= threshold }\n",
        "    get_fre_singletons = sorted(c_singletons)\n",
        "\n",
        "    # Initialize results with singletons\n",
        "    localRes.extend(get_fre_singletons)\n",
        "    k=2\n",
        "    items_fre = set(get_fre_singletons)\n",
        "\n",
        "    # Generate candidate itemsets of increasing size until no more frequent itemsets are found\n",
        "    while len(items_fre) != 0:\n",
        "        if k==2:\n",
        "            pairs = list()\n",
        "\n",
        "            # Generate pairs of items for k=2\n",
        "            for val in combinations(items_fre, 2):\n",
        "                val = list(val)\n",
        "                val.sort()\n",
        "                pairs.append(val)\n",
        "            candidate_k = pairs\n",
        "\n",
        "        else:\n",
        "            # Generate candidate itemsets of size k > 2\n",
        "            perm = list()\n",
        "            items_fre = list(items_fre)\n",
        "            for i in range(len(items_fre)-1):\n",
        "                for j in range(i+1, len(items_fre)):\n",
        "                    a = items_fre[i]\n",
        "                    b = items_fre[j]\n",
        "                    if a[0:(k-2)] == b[0:(k-2)]:\n",
        "                        perm.append(list(set(a) | set(b)))\n",
        "                    else:\n",
        "                        break\n",
        "            candidate_k = perm\n",
        "\n",
        "        # Count occurrences of k-itemsets and filter by local support\n",
        "        k_item_Dict = {}\n",
        "        for candidate in candidate_k:\n",
        "            candidate = set(candidate)\n",
        "            key = tuple(sorted(candidate))\n",
        "            for basket in baskets:\n",
        "                if candidate.issubset(basket):\n",
        "                    if key in k_item_Dict:\n",
        "                        k_item_Dict[key] = k_item_Dict[key] + 1\n",
        "                    else:\n",
        "                        k_item_Dict[key] = 1\n",
        "        kItem = Counter(k_item_Dict)\n",
        "        k_fre_items = {x : kItem[x] for x in kItem if kItem[x] >= threshold }\n",
        "        k_fre_items = sorted(k_fre_items)\n",
        "        new_item_fre = k_fre_items\n",
        "\n",
        "        # Update results and prepare for next iteration\n",
        "        localRes.extend(new_item_fre)\n",
        "        items_fre = list(set(new_item_fre))\n",
        "        items_fre.sort()\n",
        "        k=k+1\n",
        "\n",
        "    return localRes"
      ],
      "metadata": {
        "id": "LL97T01la8dP"
      },
      "execution_count": 135,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Main function implementing the SON algorithm\n",
        "def SON_ALGO():\n",
        "    start = time.time()\n",
        "\n",
        "    # Define the global support threshold\n",
        "    support  = .5\n",
        "    inputFile = \"basket.txt\"\n",
        "    outputFile = \"son_output.txt\"\n",
        "\n",
        "    # Initialize Spark context with configuration\n",
        "    conf = SparkConf().setMaster(\"local[4]\").setAppName(\"SON_DM\")\n",
        "\n",
        "    # Get or create a SparkContext with the specified configuration\n",
        "    sc = SparkContext.getOrCreate(conf)\n",
        "\n",
        "    # Load data and preprocess into baskets\n",
        "    rdd = sc.textFile(inputFile).map(lambda line: line.strip().split(','))\n",
        "    print(rdd.take(10))\n",
        "    print()\n",
        "\n",
        "    # Calculate support threshold based on actual data size\n",
        "    chunks = rdd\n",
        "    chunksCount = chunks.count()\n",
        "    support *= chunksCount\n",
        "    print(chunksCount)\n",
        "    print()\n",
        "\n",
        "    # Phase 1: Find candidate itemsets in each partition\n",
        "    map1 = chunks.mapPartitions(lambda chunk : apriori(chunk, support, chunksCount)).map(lambda x : (x, 1))\n",
        "    reduce1 = map1.reduceByKey(lambda x,y: (1)).keys().collect()\n",
        "\n",
        "    # Output candidates to file and stdout\n",
        "    print(\"Candidates:\")\n",
        "    resFile = open(outputFile, \"w\")\n",
        "    resFile.write(\"Candidates:\")\n",
        "    resFile.write(\"\\n\\n\")\n",
        "    for i in reduce1:\n",
        "        resFile.write(str(i))\n",
        "        print(str(i).strip())\n",
        "        resFile.write(',')\n",
        "    resFile.write(\"\\n\\n\")\n",
        "    print()\n",
        "\n",
        "    # Phase 2: Count and filter global frequent itemsets based on candidates from phase 1\n",
        "    map2 = chunks.mapPartitions(lambda chunk : total_c_Count(chunk, reduce1))\n",
        "    reduce2 = map2.reduceByKey(lambda x,y: (x+y))\n",
        "    finalRes = reduce2.filter(lambda x: x[1] >= support)\n",
        "    freItems = finalRes.keys().collect()\n",
        "\n",
        "    # Output frequent itemsets to file and stdout\n",
        "    print(\"Frequent Itemsets:\")\n",
        "    resFile.write(\"\\n\\n\")\n",
        "    resFile.write(\"Frequent Itemsets:\")\n",
        "    resFile.write(\"\\n\")\n",
        "    if len(freItems) != 0:\n",
        "        size = len(freItems[0])\n",
        "        j = 0\n",
        "        for i in range(0, len(freItems)):\n",
        "            c_size = len(freItems[i])\n",
        "            if size == c_size:\n",
        "                if j!=0:\n",
        "                    resFile.write(\", \")\n",
        "            else:\n",
        "                resFile.write(\"\\n\\n\")\n",
        "            if c_size == 1:\n",
        "                z = []\n",
        "                for val in freItems[i]:\n",
        "                    val = \"'\" + str(val) + \"'\"\n",
        "                    val = val.strip('\\'\"')\n",
        "                    z.append(val)\n",
        "                z = tuple(z)\n",
        "                str_val = str(z).replace(',', '')\n",
        "            else:\n",
        "                x = []\n",
        "                for val in freItems[i]:\n",
        "                    val = \"'\" + str(val) + \"'\"\n",
        "                    val = val.strip('\\'\"')\n",
        "                    x.append(val)\n",
        "                x = tuple(x)\n",
        "                str_val = str(x)\n",
        "            resFile.write(str_val); size = c_size; j = j+1\n",
        "            print(str_val)\n",
        "    end = time.time()\n",
        "    print()\n",
        "    print(\"Time taken: \")\n",
        "    print(end - start)"
      ],
      "metadata": {
        "id": "GyZPnPLXbAa3"
      },
      "execution_count": 136,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Execute the SON algorithm\n",
        "SON_ALGO()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y07X9Bm9bErA",
        "outputId": "ef786369-b0ad-4108-f37d-d9199b097380"
      },
      "execution_count": 137,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['1', '2', '4', '10', '14', '15', '16'], ['1', '2', '4', '5', '14', '16', '17'], ['1', '3', '4', '5', '6', '7', '8', '14', '17', '19'], ['1', '2', '3', '4', '5', '12', '13', '14', '15', '16'], ['1', '3', '4', '6', '7', '13', '14', '15', '16'], ['1', '2', '4', '5', '8', '9', '13', '14', '15', '16', '17', '18'], ['1', '2', '3', '6', '7', '9', '14', '16', '17'], ['1', '2', '3', '5', '8', '14', '15', '19'], ['1', '2', '3', '4', '6', '8', '13'], ['1', '3', '4', '5', '6', '12', '14', '15', '16', '18']]\n",
            "\n",
            "114520\n",
            "\n",
            "Candidates:\n",
            "1\n",
            "14\n",
            "4\n",
            "('1', '14')\n",
            "('1', '4')\n",
            "('14', '4')\n",
            "('15', '2')\n",
            "('15', '3')\n",
            "('2', '3')\n",
            "('2', '5')\n",
            "('1', '14', '15')\n",
            "('1', '14', '2')\n",
            "('1', '14', '3')\n",
            "('1', '14', '5')\n",
            "('1', '2', '4')\n",
            "('1', '3', '4')\n",
            "('14', '2', '4')\n",
            "('1', '14', '2', '3')\n",
            "('1', '2', '3', '4')\n",
            "15\n",
            "2\n",
            "3\n",
            "5\n",
            "('1', '15')\n",
            "('1', '2')\n",
            "('1', '3')\n",
            "('1', '5')\n",
            "('14', '15')\n",
            "('14', '2')\n",
            "('14', '3')\n",
            "('14', '5')\n",
            "('2', '4')\n",
            "('3', '4')\n",
            "('1', '14', '4')\n",
            "('1', '15', '2')\n",
            "('1', '15', '3')\n",
            "('1', '2', '3')\n",
            "('1', '2', '5')\n",
            "('14', '2', '3')\n",
            "('2', '3', '4')\n",
            "('1', '14', '2', '4')\n",
            "\n",
            "Frequent Itemsets:\n",
            "('1')\n",
            "('1', '4')\n",
            "('4')\n",
            "('1', '14')\n",
            "('1', '4')\n",
            "('14', '4')\n",
            "('15', '2')\n",
            "('15', '3')\n",
            "('2', '3')\n",
            "('2', '5')\n",
            "('1', '14', '15')\n",
            "('1', '14', '2')\n",
            "('1', '14', '3')\n",
            "('1', '14', '5')\n",
            "('1', '2', '4')\n",
            "('1', '3', '4')\n",
            "('14', '2', '4')\n",
            "('1', '14', '2', '3')\n",
            "('1', '2', '3', '4')\n",
            "('1', '5')\n",
            "('2')\n",
            "('3')\n",
            "('5')\n",
            "('1', '15')\n",
            "('1', '2')\n",
            "('1', '3')\n",
            "('1', '5')\n",
            "('14', '15')\n",
            "('14', '2')\n",
            "('14', '3')\n",
            "('14', '5')\n",
            "('2', '4')\n",
            "('3', '4')\n",
            "('1', '14', '4')\n",
            "('1', '15', '2')\n",
            "('1', '15', '3')\n",
            "('1', '2', '3')\n",
            "('1', '2', '5')\n",
            "('14', '2', '3')\n",
            "('2', '3', '4')\n",
            "('1', '14', '2', '4')\n",
            "\n",
            "Time taken: \n",
            "10.593577146530151\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Toivonen's Algorithm**\n",
        "The Toivonen algorithm introduces a probabilistic approach to frequent itemset mining. It starts by selecting a random sample of the dataset and then applies the Apriori algorithm to this sample to find potential frequent itemsets and generate a \"negative border\" – itemsets that are not frequent in the sample but are close to the threshold. The entire dataset is then scanned to verify which of the sampled frequent itemsets are genuinely frequent and to ensure that no itemsets in the negative border are frequent. This method reduces the computational cost by potentially requiring only one full scan of the dataset, at the cost of having to handle the complexity of dealing with the negative border.\n",
        "\n",
        "## **Working of Toivonen's Algorithm**\n",
        "\n",
        "## First Pass\n",
        "Find candidate frequent itemsets from sample\n",
        "*   *Use lower threshold* : For fraction *p* of baskets in sample, use *0.8ps* or *0.9ps* as support threshold - identifies itemsets that are frequent for the sample\n",
        "*   Construct the *negative border* - itemsets that are not frequent in the sample but all of their immediate subsets are frequent\n",
        "\n",
        "## Second Pass\n",
        "Process the whole file (no sampling)\n",
        "*   Count all candidate frequent itemsets from the first pass and all itemsets on the negative border\n",
        "*   *Case 1* : No itemset from the negative border turns out to be frequent in the whole data set - correct set of frequent itemsets is exactly the itemsets from the sample that were found frequent in the whole data\n",
        "*   *Case 2* : Some member of negative border is frequent in the whole data set - can give no answer at this time and must repeat the algorithm with a new random sample\n",
        "\n",
        "## **Why Toivonen's Algorithm Works**\n",
        "Toivonen’s algorithm never constructs a false positive, since it only describes as frequent those itemsets that have been counted and found to be frequent in the total. It also never constructs a false negative, as when no itemset of the negative border is frequent in the whole, there can be no itemset that is both frequent in the complete itemset and present in neither the negative border nor the collection of frequent itemsets for the given sample."
      ],
      "metadata": {
        "id": "hkOGSEdymWtS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import re\n",
        "import random\n",
        "import time"
      ],
      "metadata": {
        "id": "SIK0iuQjmaSy"
      },
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This function executes the second pass of the Toivonen algorithm.\n",
        "# It validates the candidate itemsets discovered in the first pass against the entire dataset.\n",
        "def ExecuteSecondPassToivonen(lstRandomFrequentItemSets, lstNegativeBorderItemSets, lstInputData, support):\n",
        "        # Initialize dictionaries to count occurrences of candidate and negative border itemsets\n",
        "        dictFrequentItems= {}\n",
        "        dictNegativeBorderItemCounts = {}\n",
        "\n",
        "        # Initialize lists to hold final frequent itemsets and negative border itemsets exceeding support\n",
        "        lstNegativeBorder= []\n",
        "        lstFinalFrequentItems = []\n",
        "\n",
        "        # Count occurrences of each candidate itemset in the entire dataset\n",
        "        for setItem in lstRandomFrequentItemSets:\n",
        "            countItem = 0\n",
        "            for sinList in lstInputData:\n",
        "                if(set(setItem).issubset(set(sinList))):\n",
        "                    countItem += 1\n",
        "            dictFrequentItems[tuple(setItem)] = countItem\n",
        "\n",
        "        # Count occurrences of each negative border itemset in the entire dataset\n",
        "        for setNegItem in lstNegativeBorderItemSets:\n",
        "            countNegItem = 0\n",
        "            for sinLi in lstInputData:\n",
        "                if(set(setNegItem).issubset(set(sinLi))):\n",
        "                    countNegItem +=1\n",
        "            dictNegativeBorderItemCounts[tuple(setNegItem)]=countNegItem\n",
        "\n",
        "        # Add itemsets to the final list if their count exceeds the support threshold\n",
        "        for ele in dictFrequentItems:\n",
        "            if(dictFrequentItems[ele] >= support):\n",
        "                lstFinalFrequentItems.append(list(ele))\n",
        "\n",
        "        # Check for negative border itemsets that exceed the support threshold\n",
        "        for elem in dictNegativeBorderItemCounts:\n",
        "            if(dictNegativeBorderItemCounts[elem] >= support):\n",
        "                lstNegativeBorder.append(list(elem))\n",
        "\n",
        "        # Determine if the algorithm needs to be executed again based on negative border results\n",
        "        if(len(lstNegativeBorder) == 0):\n",
        "            performAlgoAgain = 0\n",
        "        else:\n",
        "            performAlgoAgain = 1\n",
        "\n",
        "        # Return a flag indicating if another execution is necessary, and the list of frequent itemsets\n",
        "        return performAlgoAgain, lstFinalFrequentItems"
      ],
      "metadata": {
        "id": "jVS-cEISmrUo"
      },
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This function generates frequent random itemsets from singletons based on the support threshold\n",
        "def GenerateFrequentRandomItemSets(lstFreqSingleTons, support, maxLengthTransaction, lstInputData, lstNegativeBorderItems):\n",
        "    lstFreqItems = []\n",
        "    lstFreqItems = lstFreqSingleTons\n",
        "    lstFreqAllItems = lstFreqSingleTons\n",
        "    dictCountItemSets = {}\n",
        "\n",
        "    # Generate candidate itemsets and count their occurrences in the dataset\n",
        "    while(len(lstFreqItems)>0):\n",
        "        for i in range(maxLengthTransaction):\n",
        "            for eleInd1 in range(len(lstFreqItems)):\n",
        "                for eleInd2 in range(eleInd1+1, len(lstFreqItems)):\n",
        "                    setA = set(lstFreqItems[eleInd1]+lstFreqItems[eleInd2])\n",
        "                    sortedList = sorted(list(setA))\n",
        "                    if(len(sortedList) == len(lstFreqItems[eleInd1])+1):\n",
        "                        count =0\n",
        "                        for sinTrans in lstInputData:\n",
        "                            if(set(sortedList).issubset(set(sinTrans))):\n",
        "                                count += 1\n",
        "                        dictCountItemSets[tuple(sortedList)] = count\n",
        "\n",
        "        # Determine if candidates meet the support threshold\n",
        "        lstFreqItems = []\n",
        "        for item in dictCountItemSets:\n",
        "            if(dictCountItemSets[item]>= support):\n",
        "                lstFreqItems.append(item)\n",
        "                lstFreqAllItems.append(list(item))\n",
        "            else:\n",
        "                lstNegativeBorderItems.append(list(item))\n",
        "\n",
        "        dictCountItemSets.clear()\n",
        "\n",
        "\n",
        "    return lstFreqAllItems, lstNegativeBorderItems"
      ],
      "metadata": {
        "id": "TeTtWTTMmzj_"
      },
      "execution_count": 111,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This function prepares input data for the Apriori algorithm by sampling and counting singletons\n",
        "def AprioriSampleInputData(lstRandInputData, support, fractionOfTransactionUsed):\n",
        "    dictSingleItemCount = {}\n",
        "    lstLengthOfLst = []\n",
        "    lstAllFrequentItem = []\n",
        "    lstNegativeBorderItems = []\n",
        "    lstNegativeSingleItems= []\n",
        "\n",
        "    # Adjust the support threshold based on the fraction of transactions used\n",
        "    randSupport = int(0.8*fractionOfTransactionUsed*support)\n",
        "    lstSingleRandFrequentItems = []\n",
        "\n",
        "    # Count occurrences of each item in the sampled data\n",
        "    for alist in lstRandInputData:\n",
        "        lstLengthOfLst.append(len(alist))\n",
        "        for item in alist:\n",
        "            dictSingleItemCount[item] = dictSingleItemCount.get(item, 0)+1\n",
        "\n",
        "    # Filter items by the adjusted support threshold\n",
        "    for element in dictSingleItemCount:\n",
        "        if(dictSingleItemCount[element] >= randSupport):\n",
        "            lstSingleRandFrequentItems.append(list(element))\n",
        "        else:\n",
        "            lstNegativeSingleItems.append(list(element))\n",
        "\n",
        "    # Generate frequent itemsets and negative borders from the filtered singletons\n",
        "    lstAllFrequentItem,lstNegativeBorderItems=GenerateFrequentRandomItemSets(sorted(lstSingleRandFrequentItems), randSupport, max(lstLengthOfLst), lstRandInputData,lstNegativeSingleItems)\n",
        "    return lstAllFrequentItem, lstNegativeBorderItems"
      ],
      "metadata": {
        "id": "xNJnyGQum2bw"
      },
      "execution_count": 112,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This function implements the first pass of the Toivonen algorithm\n",
        "# It processes the input data to find candidate frequent itemsets and their negative border\n",
        "def ExecuteFirstPassToivonen(inputData, support,fractionOfTransactionUsed):\n",
        "    # Initialize lists to store preprocessed input data and lengths of transactions\n",
        "    lstInputData = []\n",
        "    lstLengthOfEachTrasaction = []\n",
        "\n",
        "    # Initialize lists to store frequent itemsets and negative border itemsets found in the sample\n",
        "    lstFreqRandItems = []\n",
        "    lstNegativeBorderItems = []\n",
        "\n",
        "    # Preprocess each line of the input data to extract items and sort them\n",
        "    # 're.findall('\\w', line)' extracts all alphanumeric characters as separate items\n",
        "    for line in inputData:\n",
        "        # Extract items from each line\n",
        "        line = re.findall('\\w',line)\n",
        "\n",
        "        # Sort the items alphabetically\n",
        "        lines = sorted(line)\n",
        "\n",
        "        # Record the length of each transaction\n",
        "        lstLengthOfEachTrasaction.append(len(lines))\n",
        "\n",
        "        # Add the sorted list of items to the input data list\n",
        "        lstInputData.append(lines)\n",
        "\n",
        "    # Calculate the number of elements to use in the sample based on the specified fraction\n",
        "    numberOfElements = len(lstInputData)*fractionOfTransactionUsed\n",
        "\n",
        "    # Randomly select a sample of transactions from the input data\n",
        "    lstRandInputData = random.sample(lstInputData,int(numberOfElements))\n",
        "\n",
        "    # Apply the Apriori algorithm to the sampled data to find frequent itemsets and their negative border\n",
        "    # This step leverages the probabilistic nature of the Toivonen algorithm to efficiently find candidate itemsets\n",
        "    lstFreqRandItems, lstNegativeBorderItems = AprioriSampleInputData(lstRandInputData, support, fractionOfTransactionUsed)\n",
        "\n",
        "    # Return the frequent itemsets, negative border itemsets, and preprocessed input data for further processing\n",
        "    return lstFreqRandItems, lstNegativeBorderItems, lstInputData"
      ],
      "metadata": {
        "id": "N1s2YUbXm681"
      },
      "execution_count": 113,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This function is the entry point for executing the Toivonen algorithm.\n",
        "# It orchestrates the execution of the algorithm's passes and outputs the results\n",
        "def Toivonen():\n",
        "    start=time.time()\n",
        "\n",
        "    inputData=\"basket2.txt\"\n",
        "    support = 5\n",
        "    fractionOfTransactionUsed = 0.4\n",
        "    numberOfIterations =0\n",
        "    executeToiven = 1\n",
        "    lstRandomFrequentItemSets = []\n",
        "    lstNegativeBorderItemSets = []\n",
        "    lstFinalFrequentItems = []\n",
        "    lstInputData= []\n",
        "    dictSortedList = {}\n",
        "\n",
        "    # Repeatedly execute the algorithm until no new negative borders are found\n",
        "    while(executeToiven > 0):\n",
        "        numberOfIterations +=1\n",
        "\n",
        "        # Process the input data and execute the first pass of Toivonen\n",
        "        inputData=open(inputData)\n",
        "        lstRandomFrequentItemSets, lstNegativeBorderItemSets, lstInputData = ExecuteFirstPassToivonen(inputData, support,fractionOfTransactionUsed)\n",
        "\n",
        "        # Execute the second pass to validate the frequent itemsets\n",
        "        executeToiven, lstFinalFrequentItems=ExecuteSecondPassToivonen(lstRandomFrequentItemSets, lstNegativeBorderItemSets, lstInputData, support)\n",
        "        inputData.close()\n",
        "\n",
        "    # Output the number of iterations and the fraction of data used\n",
        "    print(numberOfIterations)\n",
        "    print(fractionOfTransactionUsed)\n",
        "\n",
        "    # Sort and display the final frequent itemsets\n",
        "    lstFinalFrequentItems = sorted(lstFinalFrequentItems)\n",
        "\n",
        "    for item in lstFinalFrequentItems:\n",
        "        dictSortedList.setdefault(len(item),[]).append(item)\n",
        "\n",
        "    for lengthItem in dictSortedList:\n",
        "        print(list(dictSortedList[lengthItem]))\n",
        "\n",
        "    end = time.time()\n",
        "    print()\n",
        "    print(\"Time taken: \")\n",
        "    print(end - start)"
      ],
      "metadata": {
        "id": "1anOLoj2m-2C"
      },
      "execution_count": 114,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Main function call to start the Toivonen algorithm\n",
        "Toivonen()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qk7FSE1WnCnB",
        "outputId": "0cd26c6e-1bf5-4ef2-f301-8ad213d94c22"
      },
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n",
            "0.4\n",
            "[['1'], ['2'], ['3'], ['4']]\n",
            "[['1', '2'], ['1', '3'], ['2', '3']]\n",
            "\n",
            "Time taken: \n",
            "0.004271984100341797\n"
          ]
        }
      ]
    }
  ]
}